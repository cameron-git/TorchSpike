{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from spikingjelly.activation_based import neuron as snn, surrogate as sg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.1987,  1.4351,  0.4654,  1.3688]],\n",
       "\n",
       "        [[-1.2085, -1.5836, -2.6538,  0.7185]],\n",
       "\n",
       "        [[ 0.1599, -0.9066, -1.1468, -0.9280]],\n",
       "\n",
       "        [[-0.6140,  0.3344, -0.4832,  0.2014]],\n",
       "\n",
       "        [[-0.1100,  1.3366,  2.1782,  0.4904]],\n",
       "\n",
       "        [[-1.8840, -1.9816,  1.3575,  0.0211]],\n",
       "\n",
       "        [[-0.3933,  0.2061, -0.3247, -0.5327]],\n",
       "\n",
       "        [[-0.6359, -0.5674,  0.0686, -0.3607]]], requires_grad=True)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(8, 1, 4, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LIF2(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x0, v0, v_threshold, tau):\n",
    "        v1 = v0 + (x0 - v0) / tau\n",
    "        x1 = (v1 >= v_threshold).float()\n",
    "        v2 = v1 * (1 - x1)\n",
    "        ctx.save_for_backward(x1, v1, v_threshold, tau)\n",
    "        return x1, v2\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_x, grad_v):\n",
    "        x1, v1, v_threshold, tau = ctx.saved_tensors\n",
    "        grad_x = grad_x + grad_v * -v1\n",
    "        grad_v = grad_v * (1 - x1) + grad_x * torch.sigmoid(v1 - v_threshold) * (\n",
    "            1 - torch.sigmoid(v1 - v_threshold)\n",
    "        )  # (1 / ((v1 - v_threshold) ** 2 + 1))\n",
    "        grad_x = grad_v * (1 / tau)\n",
    "        grad_v = grad_v * (1 - 1 / tau)\n",
    "        return grad_x, grad_v, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0994,  0.7175,  0.2327,  0.6844]],\n",
      "       grad_fn=<DifferentiableGraphBackward>)\n",
      "tensor([[-1.1539, -0.4331, -1.2105,  0.7015]],\n",
      "       grad_fn=<DifferentiableGraphBackward>)\n",
      "tensor([[-0.4970, -0.6698, -1.1787, -0.1133]],\n",
      "       grad_fn=<DifferentiableGraphBackward>)\n",
      "tensor([[-0.5555, -0.1677, -0.8309,  0.0441]],\n",
      "       grad_fn=<DifferentiableGraphBackward>)\n",
      "tensor([[-0.3328,  0.5844,  0.6736,  0.2672]],\n",
      "       grad_fn=<DifferentiableGraphBackward>)\n",
      "tensor([[-1.1084, -0.6986,  0.0000,  0.1441]],\n",
      "       grad_fn=<DifferentiableGraphBackward>)\n",
      "tensor([[-0.7508, -0.2462, -0.1624, -0.1943]],\n",
      "       grad_fn=<DifferentiableGraphBackward>)\n",
      "tensor([[-0.6934, -0.4068, -0.0469, -0.2775]],\n",
      "       grad_fn=<DifferentiableGraphBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[0., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 1., 0.]],\n",
       " \n",
       "         [[0., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., 0.]]], grad_fn=<StackBackward0>),\n",
       " tensor([[[0.0038, 0.0060, 0.0051, 0.0064]],\n",
       " \n",
       "         [[0.0041, 0.0052, 0.0036, 0.0063]],\n",
       " \n",
       "         [[0.0048, 0.0052, 0.0040, 0.0060]],\n",
       " \n",
       "         [[0.0046, 0.0057, 0.0047, 0.0061]],\n",
       " \n",
       "         [[0.0043, 0.0056, 0.0052, 0.0059]],\n",
       " \n",
       "         [[0.0032, 0.0042, 0.0033, 0.0053]],\n",
       " \n",
       "         [[0.0031, 0.0040, 0.0044, 0.0042]],\n",
       " \n",
       "         [[0.0021, 0.0025, 0.0030, 0.0027]]]))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lif1 = snn.LIFNode(surrogate_function=sg.Sigmoid(alpha=1))\n",
    "x.grad = None\n",
    "out = []\n",
    "for xt in x:\n",
    "    out += [lif1(xt)]\n",
    "    print(lif1.v)\n",
    "out = torch.stack(out)\n",
    "out.mean().backward()\n",
    "out, x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0994,  0.7175,  0.2327,  0.6844]], grad_fn=<LIF2Backward>)\n",
      "tensor([[-1.1539, -0.4331, -1.2105,  0.7015]], grad_fn=<LIF2Backward>)\n",
      "tensor([[-0.4970, -0.6698, -1.1787, -0.1133]], grad_fn=<LIF2Backward>)\n",
      "tensor([[-0.5555, -0.1677, -0.8309,  0.0441]], grad_fn=<LIF2Backward>)\n",
      "tensor([[-0.3328,  0.5844,  0.6736,  0.2672]], grad_fn=<LIF2Backward>)\n",
      "tensor([[-1.1084, -0.6986,  0.0000,  0.1441]], grad_fn=<LIF2Backward>)\n",
      "tensor([[-0.7508, -0.2462, -0.1624, -0.1943]], grad_fn=<LIF2Backward>)\n",
      "tensor([[-0.6934, -0.4068, -0.0469, -0.2775]], grad_fn=<LIF2Backward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[0., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 1., 0.]],\n",
       " \n",
       "         [[0., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., 0.]]], grad_fn=<StackBackward0>),\n",
       " tensor([[[0.0038, 0.0060, 0.0051, 0.0064]],\n",
       " \n",
       "         [[0.0041, 0.0052, 0.0036, 0.0063]],\n",
       " \n",
       "         [[0.0048, 0.0052, 0.0040, 0.0060]],\n",
       " \n",
       "         [[0.0046, 0.0057, 0.0047, 0.0061]],\n",
       " \n",
       "         [[0.0043, 0.0056, 0.0052, 0.0059]],\n",
       " \n",
       "         [[0.0032, 0.0042, 0.0033, 0.0053]],\n",
       " \n",
       "         [[0.0031, 0.0040, 0.0044, 0.0042]],\n",
       " \n",
       "         [[0.0021, 0.0025, 0.0030, 0.0027]]]))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lif2 = LIF2.apply\n",
    "v=torch.zeros_like(x[0])\n",
    "x.grad = None\n",
    "v.grad = None\n",
    "v_threshold = torch.tensor(1.0)\n",
    "tau = torch.tensor(2)\n",
    "out = []\n",
    "for xt in x:\n",
    "    spike, v = lif2(xt, v, v_threshold, tau)\n",
    "    print(v)\n",
    "    out += [spike]\n",
    "out = torch.stack(out)\n",
    "out.mean().backward()\n",
    "out, x.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

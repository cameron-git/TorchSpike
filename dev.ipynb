{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import timeit\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "runs = 20\n",
    "neurons = [1024, 2048, 4096, 8192, 16384]\n",
    "x = torch.rand(500, 16, neurons[3], requires_grad=True).to(device)\n",
    "x = torch.rand(8, 1, 4, requires_grad=True).to(device)\n",
    "x.retain_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\TorchSpike\\.venv\\lib\\site-packages\\torch\\cuda\\memory.py:329: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.008588854999999996,\n",
       " 0.00390625,\n",
       " tensor([[[0., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., 1.]],\n",
       " \n",
       "         [[1., 1., 1., 0.]],\n",
       " \n",
       "         [[0., 0., 0., 1.]],\n",
       " \n",
       "         [[1., 1., 1., 0.]],\n",
       " \n",
       "         [[0., 0., 0., 0.]],\n",
       " \n",
       "         [[1., 0., 1., 1.]],\n",
       " \n",
       "         [[0., 1., 0., 0.]]], device='cuda:0', grad_fn=<ViewBackward0>),\n",
       " tensor([[[ 0.0163,  0.0171,  0.0165,  0.0108]],\n",
       " \n",
       "         [[ 0.0104,  0.0111,  0.0102,  0.0038]],\n",
       " \n",
       "         [[ 0.0034,  0.0042,  0.0032,  0.0096]],\n",
       " \n",
       "         [[ 0.0108,  0.0073,  0.0103,  0.0030]],\n",
       " \n",
       "         [[ 0.0042, -0.0007,  0.0032,  0.0176]],\n",
       " \n",
       "         [[ 0.0121,  0.0194,  0.0125,  0.0114]],\n",
       " \n",
       "         [[ 0.0054,  0.0137,  0.0058,  0.0047]],\n",
       " \n",
       "         [[ 0.0072,  0.0077,  0.0074,  0.0071]]], device='cuda:0'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SpikingJelly\n",
    "torch.cuda.reset_max_memory_allocated()\n",
    "from spikingjelly.activation_based import (\n",
    "    neuron as snn,\n",
    "    surrogate as sg,\n",
    "    functional as sf,\n",
    ")\n",
    "\n",
    "step_mode = \"m\"\n",
    "backend = \"cupy\"\n",
    "lif = snn.IFNode(\n",
    "    surrogate_function=sg.Sigmoid(alpha=1.0), step_mode=step_mode, v_reset=0.0\n",
    ").to(device)\n",
    "sf.set_backend(lif, backend)\n",
    "\n",
    "\n",
    "def run():\n",
    "    global out\n",
    "    lif.reset()\n",
    "    x.grad = None\n",
    "    if step_mode == \"m\":\n",
    "        out = lif(x)\n",
    "    else:\n",
    "        out = []\n",
    "        for xt in x:\n",
    "            out += [lif(xt)]\n",
    "        out = torch.stack(out)\n",
    "    out.mean().backward()\n",
    "\n",
    "\n",
    "result = timeit.timeit(run, number=runs)\n",
    "result / runs, torch.cuda.max_memory_allocated() / 1024**2, out, x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\TorchSpike\\.venv\\lib\\site-packages\\torch\\cuda\\memory.py:329: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.003124664999999993,\n",
       " 0.01513671875,\n",
       " tensor([[[0., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., 1.]],\n",
       " \n",
       "         [[1., 1., 1., 0.]],\n",
       " \n",
       "         [[0., 0., 0., 1.]],\n",
       " \n",
       "         [[1., 1., 1., 0.]],\n",
       " \n",
       "         [[0., 0., 0., 0.]],\n",
       " \n",
       "         [[1., 0., 1., 1.]],\n",
       " \n",
       "         [[0., 1., 0., 0.]]], device='cuda:0', grad_fn=<StackBackward0>),\n",
       " tensor([[[ 0.0163,  0.0171,  0.0165,  0.0108]],\n",
       " \n",
       "         [[ 0.0104,  0.0111,  0.0102,  0.0038]],\n",
       " \n",
       "         [[ 0.0034,  0.0042,  0.0032,  0.0096]],\n",
       " \n",
       "         [[ 0.0108,  0.0073,  0.0103,  0.0030]],\n",
       " \n",
       "         [[ 0.0042, -0.0007,  0.0032,  0.0176]],\n",
       " \n",
       "         [[ 0.0121,  0.0194,  0.0125,  0.0114]],\n",
       " \n",
       "         [[ 0.0054,  0.0137,  0.0058,  0.0047]],\n",
       " \n",
       "         [[ 0.0072,  0.0077,  0.0074,  0.0071]]], device='cuda:0'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function s\n",
    "torch.cuda.reset_max_memory_allocated()\n",
    "\n",
    "\n",
    "class LIF(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, v, th, tau):\n",
    "        v = v + x\n",
    "        x = (v >= th).to(x)\n",
    "        ctx.save_for_backward(x, v, th)\n",
    "        v = v * (1 - x)\n",
    "        return x, v\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_x, grad_v):\n",
    "        x, v, th = ctx.saved_tensors\n",
    "        grad_x = grad_x + grad_v * -v\n",
    "        sg = torch.sigmoid(v - th)\n",
    "        grad_v = grad_v * (1 - x) + grad_x * sg * (1 - sg)\n",
    "        grad_x = grad_v\n",
    "        return grad_x, grad_v, None, None\n",
    "\n",
    "\n",
    "lif = LIF.apply\n",
    "th = torch.tensor(1.0)\n",
    "tau = torch.tensor(2)\n",
    "\n",
    "\n",
    "def run():\n",
    "    global out\n",
    "    v = torch.zeros_like(x[0])\n",
    "    x.grad = None\n",
    "    v.grad = None\n",
    "    out = []\n",
    "    for xt in x:\n",
    "        xt, v = lif(xt, v, th, tau)\n",
    "        out += [xt]\n",
    "    out = torch.stack(out)\n",
    "    out.mean().backward()\n",
    "\n",
    "\n",
    "result = timeit.timeit(run, number=runs)\n",
    "result / runs, torch.cuda.max_memory_allocated() / 1024**2, out, x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.001877059999999986,\n",
       " 0.01513671875,\n",
       " tensor([[[0., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., 1.]],\n",
       " \n",
       "         [[1., 1., 1., 0.]],\n",
       " \n",
       "         [[0., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1., 0.]],\n",
       " \n",
       "         [[1., 0., 0., 1.]],\n",
       " \n",
       "         [[0., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 0., 0., 0.]]], device='cuda:0', grad_fn=<SubBackward0>),\n",
       " tensor([[[0.0005, 0.0015, 0.0012, 0.0005]],\n",
       " \n",
       "         [[0.0005, 0.0015, 0.0012, 0.0005]],\n",
       " \n",
       "         [[0.0005, 0.0015, 0.0012, 0.0005]],\n",
       " \n",
       "         [[0.0005, 0.0015, 0.0012, 0.0005]],\n",
       " \n",
       "         [[0.0005, 0.0015, 0.0012, 0.0005]],\n",
       " \n",
       "         [[0.0005, 0.0015, 0.0012, 0.0005]],\n",
       " \n",
       "         [[0.0005, 0.0015, 0.0012, 0.0005]],\n",
       " \n",
       "         [[0.0005, 0.0015, 0.0012, 0.0005]]], device='cuda:0'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function m\n",
    "torch.cuda.reset_accumulated_memory_stats()\n",
    "\n",
    "\n",
    "class Floor(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        ctx.save_for_backward(x)\n",
    "        return x.floor()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_x):\n",
    "        x = ctx.saved_tensors[0]\n",
    "        x = x % 1\n",
    "        sg0 = torch.sigmoid(10 * x)\n",
    "        sg1 = torch.sigmoid(10 * (x - 1))\n",
    "        grad_x = grad_x * (sg0 * (1 - sg0) + sg1 * (1 - sg1))\n",
    "        return grad_x\n",
    "\n",
    "\n",
    "class IF(nn.Module):\n",
    "    def __init__(self, th=1, tau=2):\n",
    "        super().__init__()\n",
    "        self.v = None\n",
    "        self.th = th\n",
    "        self.tau = tau\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.v = x.cumsum(0).relu()\n",
    "        x = torch.nn.functional.pad(\n",
    "            Floor.apply(self.v / self.th).to(x), (0, 0, 0, 0, 1, 0)\n",
    "        )\n",
    "        x = x[1:] - x[:-1]\n",
    "        self.v = self.v - x * self.th\n",
    "        return x\n",
    "\n",
    "\n",
    "lif = IF()\n",
    "th = torch.tensor(1.0)\n",
    "tau = torch.tensor(2)\n",
    "\n",
    "\n",
    "def run():\n",
    "    global out\n",
    "    v = torch.zeros_like(x)\n",
    "    x.grad = None\n",
    "    lif.v = None\n",
    "    out = lif(x)\n",
    "    out.mean().backward()\n",
    "\n",
    "\n",
    "result = timeit.timeit(run, number=runs)\n",
    "result / runs, torch.cuda.max_memory_allocated() / 1024**2, out, x.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
